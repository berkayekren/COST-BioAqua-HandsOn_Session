{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a10e56dc",
   "metadata": {},
   "source": [
    "# üåä Machine Learning & Deep Learning for Microbiome and Multi-omics Data\n",
    "### Training Hands-on Session\n",
    "**Date:** _2025-10-21_  \n",
    "**Author:** _Berkay Ekren_  \n",
    "**Session:** Hands-On\n",
    "\n",
    "##### ‚öôÔ∏è Required Tools-Modules\n",
    "1. Python >= 3.12\n",
    "2. pandas\n",
    "3. numpy\n",
    "4. seaborn\n",
    "5. matplotlib\n",
    "6. scikit-learn\n",
    "7. boruta\n",
    "8. xgboost\n",
    "9. lightgbm\n",
    "10. tensorflow\n",
    "11. keras (part of tensorflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b590b5",
   "metadata": {},
   "source": [
    "### üìÑ Data import for Hands-on Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b733473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Comment out to make sure only CPU is used\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get the medatada\n",
    "metadata_df = pd.read_csv(\"data/metadata.csv\", sep=\"\\t\")\n",
    "\n",
    "# Import data [1]\n",
    "microbiome_df = pd.read_csv(\"data/microbiome.csv\", sep=\"\\t\")\n",
    "metabolome_df = pd.read_csv(\"data/metabolome.csv\", sep=\"\\t\")\n",
    "\n",
    "# Uncomment the below 2 lines to see the first few rows of the dataframes to see the file structure\n",
    "#print(microbiome_df.head())\n",
    "#print(metabolome_df.head())\n",
    "\n",
    "# Check the distribution of the data with histograms\n",
    "plt.figure(figsize=(14, 5))\n",
    "sns.histplot(microbiome_df.iloc[:, 1:].values.flatten(), bins=50, color='blue', label='Microbiome', kde=True)\n",
    "sns.histplot(metabolome_df.iloc[:, 1:].values.flatten(), bins=50, color='orange', label='Metabolome', kde=True)\n",
    "plt.title('Distribution of Relative Abundance Values', fontsize=16)\n",
    "plt.xlabel('Abundance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08996bc",
   "metadata": {},
   "source": [
    "## üïê Hands-on Session 1: Machine Learning for Multi-omics Datasets\n",
    "### üìã Objectives:\n",
    "- Perform **classification** or **regression** using microbiome and omics datasets.\n",
    "- Identify **biomarkers** relevant to aquaculture species.\n",
    "\n",
    "### üîó Suggested Datasets:\n",
    "- Microbiome OTU/ASV tables\n",
    "- Metabolomics or transcriptomics profiles\n",
    "- Aquaculture phenotype or environmental metadata\n",
    "\n",
    "### üß∞ Tasks:\n",
    "1. Load and preprocess data\n",
    "2. Explore dataset (summary statistics, visualization)\n",
    "3. Early integration - Late integration methods\n",
    "4. Apply ML models (e.g., Random Forest, SVM, Gradient Boosting)\n",
    "5. Evaluate model performances\n",
    "6. Identify potential biomarkers (feature importance, SHAP, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c34a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries, uncomment as needed for regression tasks\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from boruta import BorutaPy\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold #, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier #, RandomForestRegressor\n",
    "from sklearn.svm import SVC #, SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "# from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a7f591",
   "metadata": {},
   "source": [
    "#### üß¨ Early integration in machine learning: Concatanate features before modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Strategy 1: Early Integration ---\")\n",
    "\n",
    "# 1. Set the first column as the index for each dataframe\n",
    "microbiome_features = microbiome_df.set_index(microbiome_df.columns[0])\n",
    "metabolome_features = metabolome_df.set_index(metabolome_df.columns[0])\n",
    "\n",
    "# 2. Transpose the dataframes so that rows are samples and columns are features\n",
    "X_microbiome = microbiome_features.T\n",
    "X_metabolome = metabolome_features.T\n",
    "\n",
    "# 3. Concatenate the dataframes horizontally (axis=1) to create a single feature matrix.\n",
    "# This aligns the data by sample ID (the index).\n",
    "early_integration_df = pd.concat([X_microbiome, X_metabolome], axis=1)\n",
    "\n",
    "print(\"Shape of Microbiome data (samples, features):\", X_microbiome.shape)\n",
    "print(\"Shape of Metabolome data (samples, features):\", X_metabolome.shape)\n",
    "print(\"Shape of combined data for Early Integration:\", early_integration_df.shape)\n",
    "\n",
    "print(\"\\n Early Integration DataFrame Head \")\n",
    "print(early_integration_df.head(2))\n",
    "\n",
    "# Create target variables (y) from metadata \n",
    "# 4. Set the first column (sample IDs) as the index of the metadata\n",
    "metadata_indexed = metadata_df.set_index(metadata_df.columns[0])\n",
    "\n",
    "# 5. Align metadata with the feature dataframe to ensure correct sample order\n",
    "aligned_metadata = metadata_indexed.reindex(early_integration_df.index)\n",
    "\n",
    "# 6. Create the classification target from the 'sampling_site' column\n",
    "# We use factorize to convert site names (e.g., 'SiteA', 'SiteB') into numbers (0, 1)\n",
    "y_classification, class_labels = pd.factorize(aligned_metadata[aligned_metadata.columns[1]])\n",
    "print(f\"\\nClassification target created from column: '{aligned_metadata.columns[1 ]}'\")\n",
    "print(f\"Classes found: {class_labels.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3e503",
   "metadata": {},
   "source": [
    "##### Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "print(\"\\n--- Data splitting for testing and training... ---\")\n",
    "\n",
    "\n",
    "# For regression, we'll use the factorized labels as a placeholder. In practice, replace this with a real continuous variable.\n",
    "y_regression = y_classification\n",
    "\n",
    "# Split data for classification task\n",
    "X_train_early_c, X_test_early_c, y_train_c, y_test_c = train_test_split(\n",
    "    early_integration_df, y_classification, test_size=0.2, random_state=42, stratify=y_classification\n",
    ")\n",
    "# Split data for regression task\n",
    "X_train_early_r, X_test_early_r, y_train_r, y_test_r = train_test_split(\n",
    "    early_integration_df, y_regression, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"\\nShapes of the datasets after splitting:\")\n",
    "print(\"Classification Task:\")\n",
    "print(\"X_train_early_c:\", X_train_early_c.shape, \"X_test_early_c:\", X_test_early_c.shape)\n",
    "print(\"y_train_c:\", y_train_c.shape, \"y_test_c:\", y_test_c.shape)\n",
    "print(\"\\nRegression Task:\")\n",
    "print(\"X_train_early_r:\", X_train_early_r.shape, \"X_test_early_r:\", X_test_early_r.shape)\n",
    "print(\"y_train_r:\", y_train_r.shape, \"y_test_r:\", y_test_r.shape)\n",
    "\n",
    "print(\"\\nData successfully split for training and testing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e8c44",
   "metadata": {},
   "source": [
    "##### Random Forest Importance dependent feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e656ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Importance dependent feature selection\n",
    "print(\"\\n--- Feature Selection using Random Forest Importance ---\")\n",
    "\n",
    "# 1. Define the base estimator for feature selection\n",
    "# We use a RandomForestClassifier, as its feature_importances_ attribute is what we need.\n",
    "# Using class_weight='balanced' is good practice for potentially imbalanced datasets.\n",
    "rf_for_selection = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "# 2. Use SelectFromModel to automatically select features\n",
    "# This transformer selects features based on an importance threshold.\n",
    "# By default, it uses the median importance as the threshold, which is a good starting point.\n",
    "feature_selector = SelectFromModel(estimator=rf_for_selection)\n",
    "\n",
    "# 3. Fit the selector on the original (pre-Boruta) training data\n",
    "print(\"Fitting the feature selector on the training data...\")\n",
    "# We use the 'early_integration' data before any feature selection was applied\n",
    "feature_selector.fit(X_train_early_c, y_train_c)\n",
    "\n",
    "# 4. Get the boolean mask of selected features\n",
    "selected_features_mask_rf = feature_selector.get_support()\n",
    "\n",
    "# 5. Apply the mask to get the new feature DataFrames\n",
    "X_train_selected_c = X_train_early_c.loc[:, selected_features_mask_rf]\n",
    "X_test_selected_c = X_test_early_c.loc[:, selected_features_mask_rf]\n",
    "\n",
    "# 6. Display the results\n",
    "print(f\"\\nOriginal number of features: {X_train_early_c.shape[1]}\")\n",
    "print(f\"Number of features selected by Random Forest: {X_train_selected_c.shape[1]}\")\n",
    "\n",
    "print(\"\\nNames of the first 10 selected features:\")\n",
    "print(X_train_selected_c.columns[:10].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92377e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection using Boruta\n",
    "print (\"\\n--- Feature Selection using Boruta ---\")\n",
    "\n",
    "# 1. Define the estimator\n",
    "# Boruta needs a base estimator that provides feature importances. Random Forest is perfect.\n",
    "rf_for_boruta = RandomForestClassifier(n_jobs=-1,class_weight='balanced',max_depth=5,random_state=42)\n",
    "\n",
    "# 2. Define Boruta feature selection method\n",
    "# n_estimators='auto' will let Boruta decide the number of trees and set verbose to 2 to see the progress\n",
    "boruta_selector = BorutaPy(estimator=rf_for_boruta,n_estimators='auto',verbose=2, random_state=42)\n",
    "\n",
    "# 3. Find all relevant features\n",
    "# Boruta expects numpy arrays, so we use .values\n",
    "# Note: Boruta can be slow on datasets with many features.\n",
    "boruta_selector.fit(X_train_early_c.values, y_train_c)\n",
    "\n",
    "# 4. Select the confirmed AND tentative important features\n",
    "# With small datasets, it's often useful to include tentative features.\n",
    "selected_features_mask = boruta_selector.support_ | boruta_selector.support_weak_\n",
    "\n",
    "X_train_selected_c = X_train_early_c.loc[:, selected_features_mask]\n",
    "X_test_selected_c = X_test_early_c.loc[:, selected_features_mask]\n",
    "\n",
    "# For the regression task, we'll use the same selected features for consistency\n",
    "X_train_selected_r = X_train_early_r.loc[:, selected_features_mask]\n",
    "X_test_selected_r = X_test_early_r.loc[:, selected_features_mask]\n",
    "\n",
    "print(f\"\\nOriginal number of features: {X_train_early_c.shape[1]}\")\n",
    "print(f\"Number of features selected by Boruta (Confirmed + Tentative): {X_train_selected_c.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models for Classification (Early Integration)\n",
    "# Hyperparameter Tuning with Cross-Validation for Multiple Models \n",
    "print(\"\\n--- Finding Best Model and Hyperparameters with GridSearchCV ---\")\n",
    "\n",
    "# Define the models and their parameter grids\n",
    "# Calculate scale_pos_weight for imbalanced datasets\n",
    "scale_pos_weight_value = (len(y_train_c) - sum(y_train_c)) / sum(y_train_c)\n",
    "y_train_c = y_train_c.astype(int)\n",
    "y_test_c = y_test_c.astype(int)\n",
    "\n",
    "# Models for the main loop (XGBoost is now excluded)\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42,\n",
    "                                             max_iter=1000),\n",
    "    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "    'SVC': SVC(random_state=42,\n",
    "               probability=True),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42,\n",
    "                                   verbose=-1), # verbose=-1 silences LGBM warnings\n",
    "    'XGBoost': xgb.XGBClassifier(base_score=np.sum(y_train_c == 0)/len(y_train_c),\n",
    "                                 objective='binary:logistic',\n",
    "                                 booster='gbtree',\n",
    "                                 tree_method='hist',\n",
    "                                 random_state=42,\n",
    "                                 scale_pos_weight=scale_pos_weight_value)\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'solver': ['liblinear'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [50, 75, 100, 150],\n",
    "        'max_depth': [2, 3, 5, 10],\n",
    "        'min_samples_leaf': [1, 3],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [0.01, 0.1, 1, 10, 50, 100],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [50, 75, 100, 150],\n",
    "        'max_depth': [2, 3, 5],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'class_weight': ['balanced'],\n",
    "        'num_leaves': [7, 15, 31],\n",
    "        'min_child_samples': [1, 2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 75, 100, 150],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.5],\n",
    "        'gamma': [0, 0.1, 0.5],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.5, 0.7, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.7, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set up the cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "best_score = -1\n",
    "best_model_name = \"\"\n",
    "best_estimator = None\n",
    "\n",
    "# Loop through the models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n Tuning {name} \")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[name], cv=cv_strategy, scoring='roc_auc', n_jobs=-1, verbose=0)\n",
    "    grid_search.fit(X_train_selected_c, y_train_c)\n",
    "    \n",
    "    print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validated AUC score: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_final = best_model.predict(X_test_selected_c)\n",
    "    y_proba_final = best_model.predict_proba(X_test_selected_c)[:, 1]\n",
    "    \n",
    "    # Plot Confusion Matrix, AUROC, and Feature Importance Side-by-Side\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14.4, 4))\n",
    "\n",
    "    # Plot 1: Confusion Matrix\n",
    "    cm = confusion_matrix(y_test_c, y_pred_final)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels, ax=axes[0])\n",
    "    axes[0].set_title(f'Confusion Matrix for {name}')\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    axes[0].set_ylabel('True Label')\n",
    "\n",
    "    # Plot 2: ROC Curve\n",
    "    RocCurveDisplay.from_predictions(y_test_c, y_proba_final, ax=axes[1])\n",
    "    axes[1].set_title(f'ROC Curve for {name}')\n",
    "    \n",
    "    # Plot 3: Feature Importance\n",
    "    importances = None\n",
    "    if hasattr(best_model, 'feature_importances_'):\n",
    "        importances = best_model.feature_importances_\n",
    "    elif hasattr(best_model, 'coef_'):\n",
    "        importances = best_model.coef_[0]\n",
    "\n",
    "    if importances is not None:\n",
    "        feature_names = X_train_selected_c.columns\n",
    "        forest_importances = pd.Series(importances, index=feature_names)\n",
    "        top_importances = forest_importances.abs().nlargest(15)\n",
    "        \n",
    "        top_importances.sort_values(ascending=True).plot.barh(ax=axes[2])\n",
    "        axes[2].set_title(f'Top 15 Feature Importances for {name}')\n",
    "        axes[2].set_xlabel('Importance')\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, 'Feature importances not available', ha='center', va='center')\n",
    "        axes[2].set_title(f'Feature Importances for {name}')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if grid_search.best_score_ > best_score:\n",
    "        best_score = grid_search.best_score_\n",
    "        best_model_name = name\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the Overall Best Model Found\n",
    "if best_estimator is not None:\n",
    "    print(f\"\\n Champion Model: {best_model_name} \")\n",
    "    print(f\"Best CV AUC: {best_score:.4f}\")\n",
    "\n",
    "    # Evaluate the final, champion model on the held-out test set\n",
    "    y_pred_final = best_estimator.predict(X_test_selected_c)\n",
    "    y_proba_final = best_estimator.predict_proba(X_test_selected_c)[:, 1]\n",
    "\n",
    "    final_accuracy = accuracy_score(y_test_c, y_pred_final)\n",
    "    final_auc = roc_auc_score(y_test_c, y_proba_final)\n",
    "\n",
    "    print(\"\\nPerformance of Champion Model on Test Set:\")\n",
    "    print(f\"  Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"  AUC: {final_auc:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report on Test Set:\")\n",
    "    print(classification_report(y_test_c, y_pred_final))\n",
    "\n",
    "else:\n",
    "    print(\"\\nCould not determine a best model due to errors during tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92599a5e",
   "metadata": {},
   "source": [
    "#### üß¨ Late Integration: Train models separately, then combine predictions (Decision-Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dbb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Strategy 2: Late Integration Data Splitting ---\")\n",
    "\n",
    "# Create training and test dataframes for microbiome and metabolome raw data\n",
    "microbiome_features = microbiome_df.set_index(microbiome_df.columns[0])\n",
    "metabolome_features = metabolome_df.set_index(metabolome_df.columns[0])\n",
    "\n",
    "# Transpose the dataframes so that rows are samples and columns are features\n",
    "X_microbiome = microbiome_features.T\n",
    "X_metabolome = metabolome_features.T\n",
    "\n",
    "# Create target variables (y) from metadata\n",
    "metadata_indexed = metadata_df.set_index(metadata_df.columns[0])\n",
    "\n",
    "# Align metadata with the feature dataframe to ensure correct sample order\n",
    "aligned_metadata = metadata_indexed.reindex(X_microbiome.index)\n",
    "\n",
    "# Create the classification target from the 'sampling_site' column\n",
    "y_classification, class_labels = pd.factorize(aligned_metadata[aligned_metadata.columns[1]])\n",
    "print(f\"\\nClassification target created from column: '{aligned_metadata.columns[1 ]}'\")\n",
    "print(f\"Classes found: {class_labels.tolist()}\")\n",
    "\n",
    "# Split microbiome data\n",
    "X_micro_train, X_micro_test, y_train_late, y_test_late = train_test_split(\n",
    "    X_microbiome, y_classification, test_size=0.2, random_state=42, stratify=y_classification\n",
    ")\n",
    "\n",
    "# Split metabolome data using the same indices from the first split\n",
    "# We can achieve this by splitting the metabolome data with the same parameters.\n",
    "X_metab_train, X_metab_test, _, _ = train_test_split(\n",
    "    X_metabolome, y_classification, test_size=0.2, random_state=42, stratify=y_classification\n",
    ")\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"\\n Shapes of the datasets for Late Integration \")\n",
    "print(\"Microbiome Data:\")\n",
    "print(\"X_micro_train:\", X_micro_train.shape, \"| X_micro_test:\", X_micro_test.shape)\n",
    "print(\"\\nMetabolome Data:\")\n",
    "print(\"X_metab_train:\", X_metab_train.shape, \"| X_metab_test:\", X_metab_test.shape)\n",
    "print(\"\\nTarget Variable:\")\n",
    "print(\"y_train_late:\", y_train_late.shape, \"| y_test_late:\", y_test_late.shape)\n",
    "\n",
    "print(\"\\nData successfully split for late integration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989ac388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature selection with Random Forest Importance for Late Integration datasets\n",
    "print(\"\\n--- Feature Selection using Random Forest Importance for Late Integration ---\")\n",
    "\n",
    "# Define a function to run RF-based feature selection, as we'll do it for each dataset\n",
    "def run_rf_selection(X_train, y_train, data_name=\"\"):\n",
    "    print(f\"\\nStarting Random Forest feature selection for {data_name} data...\")\n",
    "    # 1. Define the base estimator\n",
    "    rf_estimator = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "    \n",
    "    # 2. Use SelectFromModel to automatically select features based on median importance\n",
    "    feature_selector = SelectFromModel(estimator=rf_estimator, threshold='median')\n",
    "    \n",
    "    # 3. Fit the selector on the training data\n",
    "    feature_selector.fit(X_train, y_train)\n",
    "    \n",
    "    # 4. Get the boolean mask of selected features\n",
    "    selected_mask = feature_selector.get_support()\n",
    "    \n",
    "    print(f\"Random Forest selected {sum(selected_mask)} features from {X_train.shape[1]} for {data_name} data.\")\n",
    "    \n",
    "    return selected_mask\n",
    "\n",
    "# Run RF Selection on Microbiome Data\n",
    "rf_mask_micro = run_rf_selection(X_micro_train, y_train_late, \"Microbiome\")\n",
    "X_micro_train_selected = X_micro_train.loc[:, rf_mask_micro]\n",
    "X_micro_test_selected = X_micro_test.loc[:, rf_mask_micro]\n",
    "\n",
    "# Run RF Selection on Metabolome Data\n",
    "rf_mask_metab = run_rf_selection(X_metab_train, y_train_late, \"Metabolome\")\n",
    "X_metab_train_selected = X_metab_train.loc[:, rf_mask_metab]\n",
    "X_metab_test_selected = X_metab_test.loc[:, rf_mask_metab]\n",
    "\n",
    "print(\"\\n Shapes after Random Forest Feature Selection \")\n",
    "print(\"Microbiome Train:\", X_micro_train_selected.shape, \"| Microbiome Test:\", X_micro_test_selected.shape)\n",
    "print(\"Metabolome Train:\", X_metab_train_selected.shape, \"| Metabolome Test:\", X_metab_test_selected.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1cbd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature selection with Boruta for Late Integration datasets\n",
    "print(\"\\n--- Feature Selection using Boruta for Late Integration ---\")\n",
    "\n",
    "# Define a function to run Boruta, as we'll do it for each dataset\n",
    "def run_boruta_selection(X_train, y_train, data_name=\"\"):\n",
    "    print(f\"\\nStarting Boruta for {data_name} data...\")\n",
    "    # Define the estimator\n",
    "    rf_estimator = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "    \n",
    "    # Define Boruta feature selection method\n",
    "    boruta_selector = BorutaPy(\n",
    "        estimator=rf_estimator,\n",
    "        n_estimators='auto',\n",
    "        verbose=0, # Set to 2 to see progress\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Find all relevant features\n",
    "    boruta_selector.fit(X_train.values, y_train)\n",
    "    \n",
    "    # Get the mask for selected features (confirmed + tentative)\n",
    "    selected_mask = boruta_selector.support_ | boruta_selector.support_weak_\n",
    "    \n",
    "    print(f\"Boruta selected {sum(selected_mask)} features from {X_train.shape[1]} for {data_name} data.\")\n",
    "    \n",
    "    return selected_mask\n",
    "\n",
    "# Run Boruta on Microbiome Data\n",
    "boruta_mask_micro = run_boruta_selection(X_micro_train, y_train_late, \"Microbiome\")\n",
    "X_micro_train_selected = X_micro_train.loc[:, boruta_mask_micro]\n",
    "X_micro_test_selected = X_micro_test.loc[:, boruta_mask_micro]\n",
    "\n",
    "# Run Boruta on Metabolome Data\n",
    "boruta_mask_metab = run_boruta_selection(X_metab_train, y_train_late, \"Metabolome\")\n",
    "X_metab_train_selected = X_metab_train.loc[:, boruta_mask_metab]\n",
    "X_metab_test_selected = X_metab_test.loc[:, boruta_mask_metab]\n",
    "\n",
    "print(\"\\n Shapes after Boruta Feature Selection \")\n",
    "print(\"Microbiome Train:\", X_micro_train_selected.shape, \"| Microbiome Test:\", X_micro_test_selected.shape)\n",
    "print(\"Metabolome Train:\", X_metab_train_selected.shape, \"| Metabolome Test:\", X_metab_test_selected.shape)\n",
    "\n",
    "# The selected dataframes (e.g., X_micro_train_selected) are now ready for the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e799376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Late Integration (Decision-Level) with Feature-Selected Data\n",
    "print(\"\\n--- Late Integration (Decision-Level) with Feature-Selected Data and GridSearchCV ---\")\n",
    "\n",
    "# We will use the data selected by Boruta. To use RF-selected data, change the dictionaries below.\n",
    "datasets_train = {'micro': X_micro_train_selected, 'metab': X_metab_train_selected}\n",
    "datasets_test = {'micro': X_micro_test_selected, 'metab': X_metab_test_selected}\n",
    "\n",
    "scale_pos_weight_value = (len(y_train_late) - sum(y_train_late)) / sum(y_train_late)\n",
    "\n",
    "# Define the base models to be used for each dataset\n",
    "base_models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000,\n",
    "                                             random_state=42),\n",
    "    'SVC': SVC(probability=True,\n",
    "               random_state=42),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42,\n",
    "                                   verbose=-1),\n",
    "    'XGBoost': xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                 booster='gbtree',\n",
    "                                 tree_method='hist',\n",
    "                                 random_state=42)\n",
    "}\n",
    "\n",
    "param_grids_late = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'solver': ['liblinear'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': [50, 75, 100, 150],\n",
    "        'max_depth': [2, 3, 5, 10],\n",
    "        'min_samples_leaf': [1, 3],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [0.01, 0.1, 1, 10, 50, 100],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [50, 75, 100, 150],\n",
    "        'max_depth': [2, 3, 5],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'class_weight': ['balanced'],\n",
    "        'num_leaves': [7, 15, 31],\n",
    "        'min_child_samples': [1, 2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 75, 100, 150],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.5],\n",
    "        'gamma': [0, 0.1, 0.5],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.5, 0.7, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.7, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "meta_features_train = []\n",
    "meta_features_test = []\n",
    "model_performance = {}\n",
    "\n",
    "# Set up the cross-validation strategy\n",
    "cv_strategy = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Train base models with GridSearchCV, evaluate them, and generate predictions for the meta-model\n",
    "for name, model in base_models.items():\n",
    "    print(f\"\\n Evaluating Base Model: {name} \")\n",
    "    for data_type, X_train_curr in datasets_train.items():\n",
    "        print(f\"Tuning and training on {data_type} data...\")\n",
    "        \n",
    "        X_test_curr = datasets_test[data_type]\n",
    "        \n",
    "        # Tune hyperparameters for the base model\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grids_late[name], cv=cv_strategy, scoring='roc_auc', n_jobs=-1, verbose=0)\n",
    "        grid_search.fit(X_train_curr, y_train_late)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        print(f\"  Best params for {name} on {data_type}: {grid_search.best_params_}\")\n",
    "        \n",
    "        # Fit the best model on the full training data to evaluate it\n",
    "        best_model.fit(X_train_curr, y_train_late)\n",
    "        y_proba = best_model.predict_proba(X_test_curr)[:, 1]\n",
    "        \n",
    "        # Evaluate and store performance\n",
    "        auc = roc_auc_score(y_test_late, y_proba)\n",
    "        model_performance[f\"{name}_{data_type}\"] = auc\n",
    "        print(f\"  AUC for {name} on {data_type}: {auc:.4f}\")\n",
    "\n",
    "        # Plot Feature Importance for the base model\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(7.2, 4))\n",
    "        importances = None\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            importances = best_model.feature_importances_\n",
    "        elif hasattr(best_model, 'coef_'):\n",
    "            importances = best_model.coef_[0]\n",
    "\n",
    "        if importances is not None:\n",
    "            feature_names = X_train_curr.columns\n",
    "            base_importances = pd.Series(importances, index=feature_names)\n",
    "            top_importances = base_importances.abs().nlargest(15)\n",
    "            \n",
    "            top_importances.sort_values(ascending=True).plot.barh(ax=ax)\n",
    "            ax.set_title(f'Top 15 Feature Importances for {name} on {data_type}')\n",
    "            ax.set_xlabel('Importance')\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'Feature importances not available', ha='center', va='center')\n",
    "            ax.set_title(f'Feature Importances for {name} on {data_type}')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Generate cross-validated predictions for the training set (for meta-model)\n",
    "        train_preds = cross_val_predict(best_model, X_train_curr, y_train_late, cv=cv_strategy, method='predict_proba')[:, 1]\n",
    "        meta_features_train.append(train_preds)\n",
    "        \n",
    "        # Predict on the test set for the meta-model\n",
    "        meta_features_test.append(y_proba)\n",
    "\n",
    "# Identify the best-performing base model\n",
    "best_base_model_key = max(model_performance, key=model_performance.get)\n",
    "print(f\"\\n Best Performing Base Model Combination: {best_base_model_key} (AUC: {model_performance[best_base_model_key]:.4f}) \")\n",
    "\n",
    "# Combine predictions into final meta-feature matrices\n",
    "X_late_train = np.array(meta_features_train).T\n",
    "X_late_test = np.array(meta_features_test).T\n",
    "\n",
    "print(f\"\\nShape of the new feature set for the meta-model (train): {X_late_train.shape}\")\n",
    "print(f\"Shape of the new feature set for the meta-model (test): {X_late_test.shape}\")\n",
    "\n",
    "# Train and evaluate the final meta-models using all base model types with GridSearchCV\n",
    "print(\"\\n Training and evaluating the final meta-models with GridSearchCV \")\n",
    "meta_feature_names = [f\"{name}_{data_type}\" for name in base_models.keys() for data_type in datasets_train.keys()]\n",
    "\n",
    "for name, model in base_models.items():\n",
    "    print(f\"\\n Meta-Model: {name} \")\n",
    "    \n",
    "    # Tune the meta-model\n",
    "    meta_grid_search = GridSearchCV(estimator=model, param_grid=param_grids_late[name], cv=cv_strategy, scoring='roc_auc', n_jobs=-1, verbose=0)\n",
    "    meta_grid_search.fit(X_late_train, y_train_late)\n",
    "    best_meta_model = meta_grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"  Best params for {name} meta-model: {meta_grid_search.best_params_}\")\n",
    "\n",
    "    y_pred_meta = best_meta_model.predict(X_late_test)\n",
    "    y_proba_meta = best_meta_model.predict_proba(X_late_test)[:, 1]\n",
    "\n",
    "    accuracy_meta = accuracy_score(y_test_late, y_pred_meta)\n",
    "    auc_meta = roc_auc_score(y_test_late, y_proba_meta)\n",
    "\n",
    "    print(\"\\n Performance of Late Integration (Decision-Level) Model \")\n",
    "    print(f\"  Accuracy: {accuracy_meta:.4f}\")\n",
    "    print(f\"  AUC: {auc_meta:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_late, y_pred_meta, target_names=class_labels, zero_division=0))\n",
    "\n",
    "    # Plot confusion matrix, AUROC, and Feature Importance for the final meta-model\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14.4, 4))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm_meta = confusion_matrix(y_test_late, y_pred_meta)\n",
    "    sns.heatmap(cm_meta, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels, ax=axes[0])\n",
    "    axes[0].set_title(f'Confusion Matrix for {name} Meta-Model')\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    axes[0].set_ylabel('True Label')\n",
    "\n",
    "    # ROC Curve\n",
    "    RocCurveDisplay.from_predictions(y_test_late, y_proba_meta, ax=axes[1])\n",
    "    axes[1].set_title(f'ROC Curve for {name} Meta-Model')\n",
    "\n",
    "    # Feature Importance\n",
    "    importances = None\n",
    "    if hasattr(best_meta_model, 'feature_importances_'):\n",
    "        importances = best_meta_model.feature_importances_\n",
    "    elif hasattr(best_meta_model, 'coef_'):\n",
    "        importances = best_meta_model.coef_[0]\n",
    "\n",
    "    if importances is not None:\n",
    "        meta_importances = pd.Series(importances, index=meta_feature_names)\n",
    "        top_importances = meta_importances.abs().nlargest(15)\n",
    "        \n",
    "        top_importances.sort_values(ascending=True).plot.barh(ax=axes[2])\n",
    "        axes[2].set_title('Top 15 Meta-Feature Importances')\n",
    "        axes[2].set_xlabel('Importance')\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, 'Feature importances not available', ha='center', va='center')\n",
    "        axes[2].set_title(f'Feature Importances for {name}')\n",
    "        axes[2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13f28f6",
   "metadata": {},
   "source": [
    "## üïê Hands-on session 2: Deep Learning for Multi-omics Datasets\n",
    "### üìã Objectives:\n",
    "- Build **deep learning** models to predict outcomes from multi-omics datasets.\n",
    "- Combine datasets (multi-view or multimodal learning).\n",
    "- Evaluate performance and interpretability.\n",
    "\n",
    "### üß¨ Suggested Frameworks:\n",
    "- TensorFlow / Keras\n",
    "- PyTorch / PyTorch Lightning\n",
    "\n",
    "### üß∞ Tasks:\n",
    "1. Prepare multi-omics datasets for modeling\n",
    "2. Define and train deep learning models\n",
    "3. Evaluate performance (accuracy, loss curves, confusion matrix)\n",
    "4. Interpret model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1d1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from boruta import BorutaPy\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d3d2f",
   "metadata": {},
   "source": [
    "##### Create dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Data Splitting for Deep Learning \")\n",
    "\n",
    "# We use the raw, unselected features for the DL model.\n",
    "microbiome_features = microbiome_df.set_index(microbiome_df.columns[0])\n",
    "metabolome_features = metabolome_df.set_index(metabolome_df.columns[0])\n",
    "\n",
    "# Transpose the dataframes so that rows are samples and columns are features\n",
    "X_microbiome = microbiome_features.T\n",
    "X_metabolome = metabolome_features.T\n",
    "\n",
    "metadata_indexed = metadata_df.set_index(metadata_df.columns[0])\n",
    "\n",
    "# Align metadata with the feature dataframe to ensure correct sample order\n",
    "aligned_metadata = metadata_indexed.reindex(X_microbiome.index)\n",
    "\n",
    "# Create the classification target from the 'sampling_site' column\n",
    "y_classification, class_labels = pd.factorize(aligned_metadata[aligned_metadata.columns[1]])\n",
    "\n",
    "# Step 1: Split into training+validation (80%) and test (20%) sets\n",
    "X_micro_train_full, X_micro_test, y_train_full, y_test_dl = train_test_split(\n",
    "    X_microbiome, y_classification, test_size=0.2, random_state=42, stratify=y_classification\n",
    ")\n",
    "X_metab_train_full, X_metab_test, _, _ = train_test_split(\n",
    "    X_metabolome, y_classification, test_size=0.2, random_state=42, stratify=y_classification\n",
    ")\n",
    "\n",
    "# Step 2: Split the full training set into a final training set and a validation set\n",
    "# This results in ~60% train, ~20% validation, 20% test of the original data\n",
    "X_micro_train, X_micro_val, y_train_dl, y_val_dl = train_test_split(\n",
    "    X_micro_train_full, y_train_full, test_size=0.25, random_state=42, stratify=y_train_full\n",
    ")\n",
    "X_metab_train, X_metab_val, _, _ = train_test_split(\n",
    "    X_metab_train_full, y_train_full, test_size=0.25, random_state=42, stratify=y_train_full\n",
    ")\n",
    "\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"\\n--- Shapes of the datasets for Deep Learning ---\")\n",
    "print(\"Microbiome Data:\")\n",
    "print(\"Train:\", X_micro_train.shape, \"| Validation:\", X_micro_val.shape, \"| Test:\", X_micro_test.shape)\n",
    "print(\"\\nMetabolome Data:\")\n",
    "print(\"Train:\", X_metab_train.shape, \"| Validation:\", X_metab_val.shape, \"| Test:\", X_metab_test.shape)\n",
    "print(\"\\nTarget Variable:\")\n",
    "print(\"Train:\", y_train_dl.shape, \"| Validation:\", y_val_dl.shape, \"| Test:\", y_test_dl.shape)\n",
    "\n",
    "print(\"\\nData successfully split for Deep Learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7ff85",
   "metadata": {},
   "source": [
    "##### Apply feature selection with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcb6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature selection with SelectFromModel\n",
    "print(\"\\n--- Feature Selection using SelectFromModel for Deep Learning ---\")\n",
    "\n",
    "# Define a function to run the selection\n",
    "def run_rf_selection(X_train, y_train, data_name=\"\"):\n",
    "    print(f\"\\nStarting RF feature selection for {data_name} data...\")\n",
    "    # Using a simple RF classifier to get feature importances\n",
    "    rf_estimator = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "    \n",
    "    # Select features where importance is greater than the median\n",
    "    # This is a good balance, not too aggressive.\n",
    "    feature_selector = SelectFromModel(estimator=rf_estimator, threshold='median')\n",
    "    \n",
    "    # Fit the selector\n",
    "    feature_selector.fit(X_train, y_train)\n",
    "    \n",
    "    selected_mask = feature_selector.get_support()\n",
    "    \n",
    "    print(f\"SelectFromModel selected {sum(selected_mask)} features from {X_train.shape[1]} for {data_name} data.\")\n",
    "    \n",
    "    return selected_mask\n",
    "\n",
    "# Run Selection on Microbiome Data\n",
    "rf_mask_micro = run_rf_selection(X_micro_train, y_train_dl, \"Microbiome\")\n",
    "X_micro_train_selected = X_micro_train.loc[:, rf_mask_micro]\n",
    "X_micro_val_selected = X_micro_val.loc[:, rf_mask_micro]\n",
    "X_micro_test_selected = X_micro_test.loc[:, rf_mask_micro]\n",
    "\n",
    "# Run Selection on Metabolome Data\n",
    "rf_mask_metab = run_rf_selection(X_metab_train, y_train_dl, \"Metabolome\")\n",
    "X_metab_train_selected = X_metab_train.loc[:, rf_mask_metab]\n",
    "X_metab_val_selected = X_metab_val.loc[:, rf_mask_metab]\n",
    "X_metab_test_selected = X_metab_test.loc[:, rf_mask_metab]\n",
    "\n",
    "print(\"\\n Shapes after SelectFromModel Feature Selection \")\n",
    "print(\"Microbiome Train:\", X_micro_train_selected.shape, \"| Validation:\", X_micro_val_selected.shape, \"| Test:\", X_micro_test_selected.shape)\n",
    "print(\"Metabolome Train:\", X_metab_train_selected.shape, \"| Validation:\", X_metab_val_selected.shape, \"| Test:\", X_metab_test_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d0206",
   "metadata": {},
   "source": [
    "##### Apply feature selection with Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d19c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature selection with Boruta this can be too sctick for some datasets.\n",
    "print(\"\\n--- Feature Selection using Boruta for Deep Learning ---\")\n",
    "\n",
    "# Define a function to run Boruta, as we'll do it for each dataset\n",
    "def run_boruta_selection(X_train, y_train, data_name=\"\"):\n",
    "    print(f\"\\nStarting Boruta for {data_name} data...\")\n",
    "    # Define the estimator\n",
    "    rf_estimator = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "    \n",
    "    # Define Boruta feature selection method\n",
    "    boruta_selector = BorutaPy(\n",
    "        estimator=rf_estimator,\n",
    "        n_estimators='auto',\n",
    "        verbose=0, # Set to 2 to see progress\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Find all relevant features\n",
    "    boruta_selector.fit(X_train.values, y_train)\n",
    "    \n",
    "    # Get the mask for selected features (confirmed + tentative)\n",
    "    selected_mask = boruta_selector.support_ | boruta_selector.support_weak_\n",
    "    \n",
    "    print(f\"Boruta selected {sum(selected_mask)} features from {X_train.shape[1]} for {data_name} data.\")\n",
    "    \n",
    "    return selected_mask\n",
    "\n",
    "# Run Boruta on Microbiome Data\n",
    "boruta_mask_micro = run_boruta_selection(X_micro_train, y_train_dl, \"Microbiome\")\n",
    "X_micro_train_selected = X_micro_train.loc[:, boruta_mask_micro]\n",
    "X_micro_val_selected = X_micro_val.loc[:, boruta_mask_micro]\n",
    "X_micro_test_selected = X_micro_test.loc[:, boruta_mask_micro]\n",
    "\n",
    "# Run Boruta on Metabolome Data\n",
    "boruta_mask_metab = run_boruta_selection(X_metab_train, y_train_dl, \"Metabolome\")\n",
    "X_metab_train_selected = X_metab_train.loc[:, boruta_mask_metab]\n",
    "X_metab_val_selected = X_metab_val.loc[:, boruta_mask_metab]\n",
    "X_metab_test_selected = X_metab_test.loc[:, boruta_mask_metab]\n",
    "\n",
    "print(\"\\n Shapes after Boruta Feature Selection \")\n",
    "print(\"Microbiome Train:\", X_micro_train_selected.shape, \"| Validation:\", X_micro_val_selected.shape, \"| Test:\", X_micro_test_selected.shape)\n",
    "print(\"Metabolome Train:\", X_metab_train_selected.shape, \"| Validation:\", X_metab_val_selected.shape, \"| Test:\", X_metab_test_selected.shape)\n",
    "\n",
    "# The selected dataframes are now ready for the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eef095",
   "metadata": {},
   "source": [
    "##### Run autoencoder feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Extraction with Autoencoders\n",
    "\n",
    "# Define the dimensionality of the encoded representation\n",
    "encoding_dim_micro = 32  # Compress microbiome features to 32 dimensions\n",
    "encoding_dim_metab = 16  # Compress metabolome features to 16 dimensions\n",
    "\n",
    "# --- Helper function to build an autoencoder ---\n",
    "def build_autoencoder(input_dim, encoding_dim, regularizer_val=1e-4):\n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder\n",
    "    # Using a regularizer can help prevent the autoencoder from just learning an identity function\n",
    "    encoded = Dense(encoding_dim, activation='relu', \n",
    "                    activity_regularizer=tf.keras.regularizers.l1(regularizer_val))(input_layer)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "    \n",
    "    # Autoencoder model (trains the whole thing)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "    # Encoder model (we'll use this to get the compressed features)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "# --- Microbiome Autoencoder ---\n",
    "print(\"\\n--- Training Microbiome Autoencoder ---\")\n",
    "autoencoder_micro, encoder_micro = build_autoencoder(X_micro_train_selected.shape[1], encoding_dim_micro)\n",
    "autoencoder_micro.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder_micro.fit(X_micro_train_selected, X_micro_train_selected,\n",
    "                      epochs=100,\n",
    "                      batch_size=8,\n",
    "                      shuffle=True,\n",
    "                      validation_data=(X_micro_val_selected, X_micro_val_selected),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "                      verbose=0) # verbose=0 to keep output clean\n",
    "\n",
    "# --- Metabolome Autoencoder ---\n",
    "print(\"--- Training Metabolome Autoencoder ---\")\n",
    "autoencoder_metab, encoder_metab = build_autoencoder(X_metab_train_selected.shape[1], encoding_dim_metab)\n",
    "autoencoder_metab.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder_metab.fit(X_metab_train_selected, X_metab_train_selected,\n",
    "                      epochs=100,\n",
    "                      batch_size=8,\n",
    "                      shuffle=True,\n",
    "                      validation_data=(X_metab_val_selected, X_metab_val_selected),\n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "                      verbose=0)\n",
    "\n",
    "# --- Use the Encoders to Transform the Data ---\n",
    "print(\"\\n--- Encoding the datasets ---\")\n",
    "X_micro_train_encoded = encoder_micro.predict(X_micro_train_selected)\n",
    "X_micro_val_encoded = encoder_micro.predict(X_micro_val_selected)\n",
    "X_micro_test_encoded = encoder_micro.predict(X_micro_test_selected)\n",
    "\n",
    "X_metab_train_encoded = encoder_metab.predict(X_metab_train_selected)\n",
    "X_metab_val_encoded = encoder_metab.predict(X_metab_val_selected)\n",
    "X_metab_test_encoded = encoder_metab.predict(X_metab_test_selected)\n",
    "\n",
    "print(\"Original microbiome shape:\", X_micro_train_selected.shape)\n",
    "print(\"Encoded microbiome shape:\", X_micro_train_encoded.shape)\n",
    "print(\"\\nOriginal metabolome shape:\", X_metab_train_selected.shape)\n",
    "print(\"Encoded metabolome shape:\", X_metab_train_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa833f8d",
   "metadata": {},
   "source": [
    "##### Build a Multi-Input Deep Learning Model using Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef40f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Define the input layers for our NEW encoded data\n",
    "input_micro_encoded = Input(shape=(encoding_dim_micro,), name='microbiome_encoded_input')\n",
    "input_metab_encoded = Input(shape=(encoding_dim_metab,), name='metabolome_encoded_input')\n",
    "\n",
    "# Concatenate the encoded feature branches\n",
    "combined = Concatenate()([input_micro_encoded, input_metab_encoded])\n",
    "\n",
    "# Fully Connected Head\n",
    "# With powerful features, the head can be simpler and more robust.\n",
    "z = Dense(16, activation='relu')(combined)\n",
    "z = Dropout(0.5)(z) # A standard dropout rate\n",
    "output = Dense(1, activation='sigmoid', name='final_output')(z)\n",
    "\n",
    "# Create and Compile the Model\n",
    "optimizer = Adam(learning_rate=0.00001)\n",
    "final_model = Model(inputs=[input_micro_encoded, input_metab_encoded], outputs=output)\n",
    "\n",
    "final_model.compile(optimizer=optimizer,\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy', 'AUC'])\n",
    "\n",
    "# Print the model summary\n",
    "final_model.summary()\n",
    "\n",
    "# Train the Model on the ENCODED data\n",
    "history = final_model.fit(\n",
    "    [X_micro_train_encoded, X_metab_train_encoded],\n",
    "    y_train_dl,\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    validation_data=([X_micro_val_encoded, X_metab_val_encoded], y_val_dl),\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the Model on the ENCODED Test Set \n",
    "print(\"\\n Evaluating the final model on the test set \")\n",
    "loss, accuracy, auc = final_model.evaluate([X_micro_test_encoded, X_metab_test_encoded], y_test_dl)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test AUC: {auc:.4f}\")\n",
    "\n",
    "# Plot Training History \n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bf9eb6",
   "metadata": {},
   "source": [
    "## üïê Hands-on 3: Integrated Workflow ‚Äî From Data to Insights\n",
    "### üìã Objectives:\n",
    "- Build an **end-to-end workflow** from raw data ‚Üí preprocessing ‚Üí ML/DL models.\n",
    "- Combine microbiome, omics, and environmental data.\n",
    "- Derive interpretable **biological insights** relevant to aquaculture.\n",
    "\n",
    "### ‚öôÔ∏è Example Workflow Steps:\n",
    "1. Raw data QC and normalization\n",
    "2. Feature selection or dimensionality reduction\n",
    "3. Model training and validation\n",
    "4. Post-hoc interpretation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0956cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Integrated pipeline pseudocode\n",
    "# Step 1: Preprocess data\n",
    "# Step 2: Train model\n",
    "# Step 3: Evaluate results\n",
    "# Step 4: Visualize findings\n",
    "\n",
    "# Placeholder for pipeline code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad80c4ae",
   "metadata": {},
   "source": [
    "### üìö Suggested Reading & Resources\n",
    "- [QIIME 2 16S Amplicon Pipeline](https://library.qiime2.org/quickstart/amplicon)\n",
    "- [QIIME 2 - MOSHPIT Whole Metagenome Pipeline](https://library.qiime2.org/quickstart/moshpit)\n",
    "- [QIIME 2 Machine Learning Plugin](https://docs.qiime2.org/)\n",
    "- [Meta-CAMP - MetaSUB Whole Metagenome Analysis Pipeline](https://github.com/Meta-CAMP)\n",
    "- [scikit-learn documentation](https://scikit-learn.org/stable/)\n",
    "- [IBM Deep Leaning](https://www.ibm.com/think/topics/deep-learning)\n",
    "- [TensorFlow tutorials](https://www.tensorflow.org/tutorials)\n",
    "- [PyTorch tutorials](https://pytorch.org/tutorials/)\n",
    "- Example datasets: [EBI Metagenomics](https://www.ebi.ac.uk/metagenomics/)\n",
    "\n",
    "**References**\n",
    "1. Mazzella, V., Dell‚ÄôAnno, A., Etxebarr√≠a, N., Gonz√°lez-Gaya, B., Nuzzo, G., Fontana, A., & N√∫√±ez-Pons, L. (2024). High microbiome and metabolome diversification in coexisting sponges with different bio-ecological traits. Communications Biology, 7(1), 422."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
